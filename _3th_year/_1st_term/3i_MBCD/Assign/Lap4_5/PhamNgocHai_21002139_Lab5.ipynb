{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lap5. Machine learning với Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo thư viện\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Toy Story (1995)')\n",
      "(2, 'GoldenEye (1995)')\n",
      "(3, 'Four Rooms (1995)')\n",
      "(4, 'Get Shorty (1995)')\n",
      "(5, 'Copycat (1995)')\n"
     ]
    }
   ],
   "source": [
    "# Load movieID và movieName từ file u.item thành một Dict\n",
    "def loadMovieNames():\n",
    "  movieNames = {}\n",
    "  with open(\"ml-100k/u.item\", encoding=\"latin1\") as f:\n",
    "    for line in f:\n",
    "      fields = line.split('|')\n",
    "      movieNames[int(fields[0])] = fields[1]\n",
    "  return movieNames\n",
    "\n",
    "movieNames = loadMovieNames()\n",
    "# print(movieNames)\n",
    "# Print 5 element of dictionary \n",
    "for idx, k in enumerate(movieNames):\n",
    "  if idx == 5: break\n",
    "  print((k, movieNames[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/29 01:46:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 01:46:59 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Sử dụng SparkSession\n",
    "\n",
    "## Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"MovieRecs\").getOrCreate()\n",
    "\n",
    "## This line is necessary on HDP 2.6.5:\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo hàm ParseInput từ file u.data \n",
    "# và convert dữ liệu thành các rows\n",
    "# và convert thành một RDD\n",
    "\n",
    "## Get the raw data\n",
    "lines = spark.read.text(\"ml-100k/u.data\").rdd\n",
    "# lines = spark.sparkContext.textFile(\"ml-100k/u.data\") # cách của lap trước\n",
    "\n",
    "## The parseInput\n",
    "def parseInput(line):\n",
    "    # fields = line.split() # thiếu .value thì lines = spark.read.text(\"ml-100k/u.data\").rdd bị lỗi vì phải đọc value của Row\n",
    "    fields = line.value.split()\n",
    "    return Row(userID = int(fields[0]), movieID = int(fields[1]), rating = int(fields[2]))\n",
    "\n",
    "## Convert it to a RDD of Row objects \n",
    "## with (userID, movieID, rating)\n",
    "ratingsRDD = lines.map(parseInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)\n",
    "# lines.foreach(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ratingsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(userID=196, movieID=242, rating=3),\n",
       " Row(userID=186, movieID=302, rating=3),\n",
       " Row(userID=22, movieID=377, rating=1),\n",
       " Row(userID=244, movieID=51, rating=2),\n",
       " Row(userID=166, movieID=346, rating=1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Chuyển đổi RDD thành dataframe và cache nó\n",
    "ratings = spark.createDataFrame(ratingsRDD).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 01:51:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/11/29 01:51:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "23/11/29 01:51:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sử dụng ALS trong Spark để train model\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userID\", itemCol=\"movieID\", ratingCol='rating')\n",
    "model = als.fit(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ratings for user ID 1:\n",
      "Three Colors: White (1994) 4\n",
      "Grand Day Out, A (1992) 3\n",
      "Desperado (1995) 4\n",
      "Glengarry Glen Ross (1992) 4\n",
      "Angels and Insects (1995) 4\n"
     ]
    }
   ],
   "source": [
    "# Hiển thị các phim mà userID = 1 đã xem\n",
    "print(\"\\nRatings for user ID 1:\")\n",
    "userRatings = ratings.filter(\"userID = 1\")\n",
    "index = 0\n",
    "for rating in userRatings.collect():\n",
    "    if index == 5: break\n",
    "    print(movieNames[rating['movieID']], rating['rating'])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(movieID=474, count=194),\n",
       " Row(movieID=29, count=114),\n",
       " Row(movieID=65, count=115),\n",
       " Row(movieID=191, count=276),\n",
       " Row(movieID=418, count=129)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In tất cả các phim được rate trên 100 lần\n",
    "ratingCounts = ratings.groupBy(\"movieID\").count().filter(\"count > 100\")\n",
    "print(type(ratingCounts))\n",
    "ratingCounts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(movieID=474, userID=1),\n",
       " Row(movieID=29, userID=1),\n",
       " Row(movieID=65, userID=1),\n",
       " Row(movieID=191, userID=1),\n",
       " Row(movieID=418, userID=1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tạo ra 1 dataframe có tên là popularMovies từ ratingCounts theo cột movieID.\n",
    "# Khởi tạo thêm 1 cột có tên là userID có giá trị = 1\n",
    "popularMovies = ratingCounts.select(\"movieID\").withColumn('userID', lit(1))\n",
    "print(type(popularMovies))\n",
    "popularMovies.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(movieID=474, userID=1, prediction=4.790917873382568),\n",
       " Row(movieID=29, userID=1, prediction=1.9533743858337402),\n",
       " Row(movieID=65, userID=1, prediction=3.744243621826172),\n",
       " Row(movieID=191, userID=1, prediction=4.196722507476807),\n",
       " Row(movieID=418, userID=1, prediction=3.472533702850342)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Biến đổi popularMovies theo model đã train \n",
    "recommendations = model.transform(popularMovies)\n",
    "\n",
    "print(type(recommendations))\n",
    "\n",
    "recommendations.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sắp xếp data giảm dần trong recommendations theo cột 'prediction'\n",
    "# để đề cử phim từ trên xuống\n",
    "from pyspark.sql.functions import desc\n",
    "# recommendations.sort('prediction', ascending = False)\n",
    "recommendations = recommendations.orderBy(desc(\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(movieID=169, userID=1, prediction=5.191006183624268),\n",
       " Row(movieID=48, userID=1, prediction=5.144643306732178),\n",
       " Row(movieID=408, userID=1, prediction=5.0885396003723145),\n",
       " Row(movieID=242, userID=1, prediction=5.073580741882324),\n",
       " Row(movieID=137, userID=1, prediction=5.020456314086914)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
